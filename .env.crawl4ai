# Crawl4AI Configuration File
# Source this file to configure Crawl4AI environment variables
# Usage: source .env.crawl4ai

# ============================================================================
# CRAWL4AI - Web Page Extraction Settings
# ============================================================================

# Enable Crawl4AI for web page extraction
export CRAWL4AI_ENABLED=1

# Minimum delay between crawl requests (seconds)
# Default: 1.0 (safe for most websites)
# Conservative: 2.0-3.0 (if getting rate limited)
# Aggressive: 0.5 (use with caution, higher IP ban risk)
export CRAWL4AI_MIN_DELAY=1.0

# Browser type for crawling
# Options: chromium, firefox, webkit
export CRAWL4AI_BROWSER_TYPE=chromium

# Run browser in headless mode (no GUI)
export CRAWL4AI_HEADLESS=true

# Enable caching of crawled pages
export CRAWL4AI_CACHE_ENABLED=true

# ============================================================================
# CRAWL BUDGET SETTINGS
# ============================================================================

# Maximum pages to crawl per missing field
# Higher = more thorough but slower
# Default: 2 (balanced)
export MAX_CRAWL_PAGES_PER_FIELD=2

# Maximum characters to extract per page
# Default: 5000 (5KB)
export CRAWL_TEXT_MAX_CHARS=5000

# ============================================================================
# FIELD-LEVEL RETRY & BACKOFF (Todo 10)
# ============================================================================

# Maximum retry attempts per field
# Default: 3
# Higher = more resilient but slower
export FIELD_SEARCH_MAX_ATTEMPTS=3

# Base backoff time for exponential backoff (seconds)
# Default: 0.5
# Actual backoff: base * (2^attempt) ± jitter
export FIELD_SEARCH_BACKOFF_BASE=0.5

# ============================================================================
# WEB SEARCH (SEARXNG) SETTINGS
# ============================================================================

# Minimum delay between search requests (seconds)
export SEARXNG_MIN_DELAY=1.0

# Enable search result caching
export SEARXNG_CACHE=1

# Enable crawling when search snippets insufficient
export SEARXNG_CRAWL=1

# ============================================================================
# IP BAN PREVENTION SUMMARY
# ============================================================================
# 
# These settings together provide 7-layer IP ban protection:
#
# Layer 1: CRAWL4AI_MIN_DELAY
#   → Enforces minimum time between requests
#
# Layer 2: robots.txt Compliance
#   → Automatic (Crawl4AI respects robots.txt)
#
# Layer 3: User-Agent Rotation
#   → Automatic (different agent per request)
#
# Layer 4: Automatic Backoff
#   → FIELD_SEARCH_BACKOFF_BASE handles retries with exponential backoff
#
# Layer 5: Rate Limiting
#   → Token bucket enforces request limits
#
# Layer 6: Smart Caching
#   → CRAWL4AI_CACHE_ENABLED prevents redundant crawls
#
# Layer 7: Graceful Degradation
#   → Falls back to search snippets if crawling fails
#
# ============================================================================
# CONFIGURATION PROFILES
# ============================================================================
#
# CONSERVATIVE MODE (Safest, Slower):
#   export CRAWL4AI_MIN_DELAY=3.0
#   export MAX_CRAWL_PAGES_PER_FIELD=1
#   export FIELD_SEARCH_MAX_ATTEMPTS=2
#
# BALANCED MODE (Recommended):
#   export CRAWL4AI_MIN_DELAY=1.0          ← Current settings
#   export MAX_CRAWL_PAGES_PER_FIELD=2
#   export FIELD_SEARCH_MAX_ATTEMPTS=3
#
# AGGRESSIVE MODE (Faster, Higher Risk):
#   export CRAWL4AI_MIN_DELAY=0.5
#   export MAX_CRAWL_PAGES_PER_FIELD=3
#   export FIELD_SEARCH_MAX_ATTEMPTS=5
#
# ============================================================================
