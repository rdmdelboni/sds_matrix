# ‚ö° Configura√ß√£o de Performance Otimizada - FDS Extractor
#
# Este arquivo cont√©m configura√ß√µes otimizadas para diferentes cen√°rios.
# Copie as configura√ß√µes desejadas para seu arquivo .env

# ==============================================================================
# üöÄ CONFIGURA√á√ÉO 1: M√ÅXIMA VELOCIDADE (Triagem R√°pida)
# ==============================================================================
# Use para: Processar grandes volumes rapidamente com qualidade razo√°vel
# Ideal para: Primeira triagem de 500+ arquivos
# Tempo estimado para 500 arquivos: 15-25 minutos

# MAX_WORKERS=8                          # M√°ximo paralelismo
# CHUNK_SIZE=2000                        # Chunks menores = mais r√°pido
# LM_STUDIO_MODEL=phi3:mini              # Modelo pequeno e r√°pido (~2GB)
# LM_STUDIO_TEMPERATURE=0.0              # Mais determin√≠stico
# LM_STUDIO_MAX_TOKENS=1000              # Respostas mais curtas
# ONLINE_SEARCH_PROVIDER=gemini          # Gemini √© mais r√°pido que LLM local
# GOOGLE_API_KEY=sua_chave_aqui          # Obtenha em https://makersuite.google.com/app/apikey

# ==============================================================================
# ‚öñÔ∏è CONFIGURA√á√ÉO 2: BALANCEADA (Velocidade + Qualidade)
# ==============================================================================
# Use para: Equil√≠brio entre velocidade e precis√£o
# Ideal para: Processamento geral, volumes m√©dios (50-300 arquivos)
# Tempo estimado para 500 arquivos: 25-40 minutos

MAX_WORKERS=4                            # Bom paralelismo
CHUNK_SIZE=4000                          # Chunks balanceados
LM_STUDIO_MODEL=llama3.2:3b              # Modelo m√©dio (~2GB)
LM_STUDIO_TEMPERATURE=0.1                # Levemente criativo
LM_STUDIO_MAX_TOKENS=1500                # Respostas moderadas
ONLINE_SEARCH_PROVIDER=gemini            # Use Gemini se dispon√≠vel
# GOOGLE_API_KEY=sua_chave_aqui

# ==============================================================================
# üéØ CONFIGURA√á√ÉO 3: M√ÅXIMA QUALIDADE (Precis√£o M√°xima)
# ==============================================================================
# Use para: Resultados finais com m√°xima precis√£o
# Ideal para: Volumes pequenos (< 50 arquivos) ou dados cr√≠ticos
# Tempo estimado para 500 arquivos: 50-90 minutos

# MAX_WORKERS=2                          # Processamento conservador
# CHUNK_SIZE=6000                        # Chunks maiores = mais contexto
# LM_STUDIO_MODEL=llama3.1:8b            # Modelo grande (~4.7GB)
# LM_STUDIO_TEMPERATURE=0.1              # Preciso
# LM_STUDIO_MAX_TOKENS=2000              # Respostas detalhadas
# ONLINE_SEARCH_PROVIDER=lmstudio        # LLM local para controle total

# ==============================================================================
# üíª CONFIGURA√á√ÉO 4: HARDWARE LIMITADO (RAM < 8GB)
# ==============================================================================
# Use para: Computadores com pouca RAM ou CPU fraca
# Ideal para: Hardware limitado
# Tempo estimado para 500 arquivos: 80-120 minutos

# MAX_WORKERS=1                          # Sem paralelismo
# CHUNK_SIZE=3000                        # Chunks m√©dios
# LM_STUDIO_MODEL=phi3:mini              # Modelo leve
# LM_STUDIO_TEMPERATURE=0.0
# LM_STUDIO_MAX_TOKENS=1000
# ONLINE_SEARCH_PROVIDER=gemini          # Recomendado para economizar recursos locais

# ==============================================================================
# ‚òÅÔ∏è CONFIGURA√á√ÉO 5: SOMENTE NUVEM (Sem LLM Local)
# ==============================================================================
# Use para: Quando n√£o tem Ollama instalado ou quer m√°xima velocidade
# Ideal para: Processamento na nuvem, sem instalar modelos locais
# Tempo estimado para 500 arquivos: 10-20 minutos
# Custo estimado: $0.10-0.30 para 500 arquivos

# MAX_WORKERS=6                          # Alto paralelismo (Gemini aguenta)
# CHUNK_SIZE=4000
# ONLINE_SEARCH_PROVIDER=gemini
# GOOGLE_API_KEY=sua_chave_aqui          # OBRIGAT√ìRIO
# GEMINI_MODEL=gemini-2.0-flash          # Modelo r√°pido
# GEMINI_TIMEOUT=60

# ==============================================================================
# üîß OUTRAS CONFIGURA√á√ïES
# ==============================================================================

# Diret√≥rio de dados (opcional - padr√£o: ./data)
# FDS_DATA_DIR=/caminho/customizado/data

# Configura√ß√£o do Ollama/LM Studio
LM_STUDIO_BASE_URL=http://127.0.0.1:11434/v1    # Endpoint Ollama
# LM_STUDIO_BASE_URL=http://127.0.0.1:1234/v1   # Endpoint LM Studio
LM_STUDIO_TIMEOUT=60

# Limites de arquivo
MAX_FILE_SIZE_MB=10                      # M√°ximo 10MB por arquivo
CHUNK_SIZE=4000                          # Tamanho dos chunks de texto

# Logging
LOG_LEVEL=INFO                           # DEBUG | INFO | WARNING | ERROR
# LOG_FILE=/caminho/customizado/app.log

# Tesseract OCR (para PDFs escaneados)
# TESSERACT_PATH=/usr/bin/tesseract     # Linux
# TESSERACT_PATH=C:\Program Files\Tesseract-OCR\tesseract.exe  # Windows

# ==============================================================================
# üìä COMO ESCOLHER A MELHOR CONFIGURA√á√ÉO
# ==============================================================================
#
# 1. Execute o benchmark:
#    python benchmark_performance.py /sua/pasta/fds --compare --files 10
#
# 2. Veja qual configura√ß√£o tem melhor throughput (arquivos/segundo)
#
# 3. Ajuste MAX_WORKERS baseado em:
#    - CPU cores: use 1-2 workers por core f√≠sico
#    - RAM: ~500MB-1GB por worker (com LLM local)
#    - VRAM: ~2-8GB dependendo do modelo LLM
#
# 4. Teste incrementalmente:
#    - Comece com 10 arquivos
#    - Depois 50 arquivos
#    - Depois volume completo
#
# ==============================================================================
# üí° DICAS DE PERFORMANCE
# ==============================================================================
#
# ‚úÖ Use Gemini para grandes volumes (muito mais r√°pido que LLM local)
# ‚úÖ Use modelos menores (phi3, llama3.2:3b) para triagem
# ‚úÖ Use modelos maiores (llama3.1:8b) apenas para dados cr√≠ticos
# ‚úÖ Aumente MAX_WORKERS at√© CPU/RAM saturar
# ‚úÖ Use SSD em vez de HDD (3-5x mais r√°pido)
# ‚úÖ Monitore recursos: htop (Linux) ou Task Manager (Windows)
# ‚úÖ Processe arquivos pequenos primeiro (ordenar por tamanho)
#
# ‚ùå N√£o use workers > n√∫cleos de CPU dispon√≠veis
# ‚ùå N√£o use modelos grandes sem RAM/VRAM suficiente
# ‚ùå N√£o processe 500 arquivos de uma vez sem testar antes com 10-50
#
# ==============================================================================
# üîó RECURSOS √öTEIS
# ==============================================================================
#
# - Instalar Ollama: https://ollama.ai
# - Modelos Ollama: https://ollama.ai/library
# - Gemini API Key: https://makersuite.google.com/app/apikey
# - Documenta√ß√£o: Ver OTIMIZACAO_PERFORMANCE.md
# - Benchmark: python benchmark_performance.py --help
#
# ==============================================================================
